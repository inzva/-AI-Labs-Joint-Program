{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 8 - Discrete Latent Variable Models and Hybrid Models - Solutions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O59IAvQZwh2d"
      },
      "source": [
        "# Week 8 - Discrete Latent Variable Models and Hybrid Models Notebook\n",
        "\n",
        "In this notebook, we will solve questions discrete latent variable models and hybrid generative models.\n",
        "\n",
        " - This notebook is prepared using PyTorch. However, you can use any Python package you want to implement the necessary functions in questions.\n",
        " - If the question asks you to implement a specific function, please do not use its readily available version from a package and implement it yourself.\n",
        "\n",
        "## Question 1\n",
        "\n",
        "Please answer the questions below:\n",
        "\n",
        "1. Please give some examples to discrete data modalities.\n",
        "1. Can we use GANs to generate discrete data points?\n",
        "1. What is REINFORCE and why do we use it?\n",
        "1. Please briefly explain Gumbel-Softmax by stating why do we need it and how do we use it in practice?\n",
        "1. Please conceptually explain how PixelVAE works.\n",
        "1. What is the novelty of $\\beta$-VAE over the classical variational auto-encoder. Please briefly explain.\n",
        "\n",
        "You can write your answer for each question in the markdown cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIybKtetzXZ0"
      },
      "source": [
        "**Please write your answer for each question here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZBfAnpdzYA4"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Implement the Gumbel-Softmax function. The function is characterized as below:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{z} = \\text{soft}\\max_i \\left(\\frac{g_i + \\log \\pi}{\\tau}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\pi$ are the class proabilities, $g_i$ are the i.i.d. samples from the gumbel distribution, and $\\tau$ is the temperature parameter $\\in (0, 1]$.\n",
        "\n",
        "You can write additional function or functions to sample from the gumbel distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkXHcZM_zYKy"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Let's assume four discrete outputs\n",
        "num_classes = 4\n",
        "logits = torch.randn(batch_size, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sTT1HetgtNA"
      },
      "source": [
        "# Alternative solution: https://gist.github.com/yzh119/fd2146d2aeb329d067568a493b20172f\n",
        "def gumbel_softmax(logits, temperature):\n",
        "  \"\"\"Applies gumbel softmax operation to the provided logits\n",
        "\n",
        "    Args:\n",
        "        logits: (N x num_classes)\n",
        "        temperature: A scalar constant that determines the bias-variance tradeoff\n",
        "    Returns:\n",
        "        the resulting tensor from the operation\n",
        "  \"\"\"\n",
        "  #######################\n",
        "  class_prob = -torch.log(-torch.log(torch.rand(logits.shape)))\n",
        "\n",
        "  return torch.nn.functional.one_hot(torch.argmax(torch.softmax((logits+class_prob)/temperature, dim=1), dim=1))\n",
        "  #######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3f08bJa54Ju"
      },
      "source": [
        "print(gumbel_softmax(logits, temperature=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC6d_ZpqgC9C"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "tensor([[0., 0., 0., 1.],\n",
        "        [0., 1., 0., 0.],\n",
        "        [0., 0., 0., 1.],\n",
        "        [0., 0., 1., 0.],\n",
        "        [0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1.],\n",
        "        [1., 0., 0., 0.],\n",
        "        [1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0.],\n",
        "        [1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0.],\n",
        "        [0., 1., 0., 0.],\n",
        "        [0., 1., 0., 0.],\n",
        "        [0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1.],\n",
        "        [0., 0., 1., 0.]])\n",
        "```\n",
        "\n",
        "**Bonus:** It is recommended for you to tinker with the temperature parameter and see how the results change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GNtF-3qDkrI"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Implement the loss function of VAE-GAN. You can refer to the [paper](https://arxiv.org/pdf/1512.09300.pdf) to see the motivation behind the loss function and the related equations.\n",
        "\n",
        "The loss function of VAE-GAN consists of three parts, first one being the KL divergence loss:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{prior} = D_{KL}(q(z|x)||p(z))\n",
        "\\end{equation}\n",
        "\n",
        "where $z$ is the latent space vector from the latent distribution $p(z)$ and $x$ is the data point to be reconstructed. Typically, $z$ is sampled from $\\mathcal N(0, 1)$. This term is considered as a regularizer and ensures that the distribution of the output of the encoder is similar to $\\mathcal N(0, 1)$.\n",
        "\n",
        "Second term is the reconstruction loss, but with a small twist:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}^{\\text{Dis}_l}_{\\text{llike}} = -\\mathbb{E}_{q(z|x)}[\\log(p(\\text{Dis}_l(x)|z)]\n",
        "\\end{equation}\n",
        "\n",
        "Equation above is the log-likelihood based reconstruction loss of the original VAE, except for $x$ is replaced by $\\text{Dis}_l(x)$. This is the intermediate represantation of the reconstructed version of $x \\sim \\text{Dec}(z)$ from the $l^{th}$ layer of the discriminator. This is to ensure that the image is not reconstructed on the pixel-level but more on a feature-level.\n",
        "\n",
        "Finally, third part of the loss is our good old GAN loss:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{\\text{GAN}} = \\log(\\text{Dis}(x)) + \\log(1 - \\text{Dis}(\\text{Gen}(z)))\n",
        "\\end{equation}\n",
        "\n",
        "The final loss of the VAE-GAN is the sum of all these losses:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L} = \\mathcal{L}_{prior} + \\mathcal{L}^{\\text{Dis}_l}_{\\text{llike}} + \\mathcal{L}_{\\text{GAN}}\n",
        "\\end{equation}\n",
        "\n",
        "Implement all three losses as different functions to the code cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQjOXT_LFzYz"
      },
      "source": [
        "mean = torch.randn(batch_size, 20)\n",
        "logvar = torch.randn(batch_size, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv6QG0R4Dk9m"
      },
      "source": [
        "def kl_loss(mean, logvar):\n",
        "  \"\"\"Calculates the KL loss based on the mean and logvar outputs of the Encoder network\n",
        "  w.r.t to the Gaussian with zero mean and unit variance\n",
        "  \n",
        "    Args:\n",
        "      mean: Tensor of mean values coming from the Encoder (N x D)\n",
        "      logvar: Tensor of log-variance values coming from the Encoder (N x D)\n",
        "    Returns:\n",
        "      The resulting KL loss\n",
        "  \"\"\"\n",
        "  #######################\n",
        "  return (-0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp()))/mean.shape[0]\n",
        "  #######################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsNaI_rGGl_T"
      },
      "source": [
        "print(kl_loss(mean, logvar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFjHXYOAVwyn"
      },
      "source": [
        "features_org = torch.randn(batch_size, 100)\n",
        "features_recon = torch.randn(batch_size, 100)\n",
        "\n",
        "# Uncomment the line below and run the function again to see a higher reconstruction error \n",
        "# features_recon = torch.normal(3, 20, batch_size, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TUcF08MVxkw"
      },
      "source": [
        "def reconstruction_loss(features_org, features_recon):\n",
        "  \"\"\"Calculates the reconstruction loss with mean squared error\n",
        "\n",
        "    Args:\n",
        "      features_org: Features of the original image obtained from an intermediate layer of the discriminator\n",
        "      features_recon: Features of the reconstructed image obtained from an intermediate layer of the discriminator\n",
        "    Returns:\n",
        "      M.S.E based reconstruction error of the features\n",
        "  \"\"\"\n",
        "  #######################\n",
        "  # Write code here\n",
        "  #######################\n",
        "  return ((features_org - features_recon)**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP37x3bCVx5G"
      },
      "source": [
        "print(reconstruction_loss(faetures_org, features_recon))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5xbksSTXAXG"
      },
      "source": [
        "outputs_real = torch.randn(batch_size, 32).clip(0, 1)\n",
        "outputs_fake = torch.randn(batch_size, 32).clip(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGcfhu98XALg"
      },
      "source": [
        "def gan_loss(d_real_outputs, d_fake_outputs):\n",
        "  \"\"\"Our good old GAN loss, doesn't need much of an explanation :)\n",
        "\n",
        "    Args:\n",
        "      d_real_outputs: Discriminator sigmoid outputs for the real data points\n",
        "      d_fake_outputs: Discriminator sigmoid outputs for the fake data points\n",
        "    Returns:\n",
        "      The calculated GAN loss\n",
        "  \"\"\"\n",
        "  #######################\n",
        "  real = torch.log(d_real_outputs + 1e-7)\n",
        "  fake = torch.log(1-d_fake_outputs + 1e-7)\n",
        "\n",
        "  return -(real + fake).mean()\n",
        "  #######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0dGR0yaW_-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a314e72-c776-4271-b1bc-4b83f0bbf2f9"
      },
      "source": [
        "print(gan_loss(outputs_real, outputs_fake))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.0524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36YEFdPi3K2e"
      },
      "source": [
        "## Bonus\n",
        "\n",
        "My master's thesis was a hybrid generative model and it was published in Pattern Recognition. I would like to briefly talk about it during the notebook session.\n",
        "\n",
        "For anyone who is interested, kindly read or skim through the paper before coming to the discussion session. I leave the link to the paper [here](https://faculty.ozyegin.edu.tr/ethemalpaydin/files/2021/01/Uras_bigan_PatRec.pdf)."
      ]
    }
  ]
}