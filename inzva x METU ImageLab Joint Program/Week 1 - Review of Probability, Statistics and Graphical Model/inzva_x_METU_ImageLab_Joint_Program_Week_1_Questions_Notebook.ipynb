{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inzva x METU ImageLab Joint Program Week 1 Questions Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsx9CxPM215j"
      },
      "source": [
        "## Q1: Theoretical Questions\n",
        "\n",
        "\n",
        "\n",
        "1.   Parametric models make some assumptions about the model parameters (e.g. Linear SVM assumes that data can be linearly seperable) while non-parametric models use data as it is in most of the cases. What can be the advantages and disadvantages of making assumptions about the data distribution?\n",
        "2.   Should we always favour discriminative models over generative ones if the classification accuracy is the metric that we care? Is there any case we can prefer generative models for classification?\n",
        "(Hint: You can check Ng et al. paper [On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes](https://papers.nips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf))\n",
        "3.   Do multiple subsequent linear layers have exactly same power as a single linear layer in deep neural networks if we do not use any nonlinear activation functions?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysGEONSgBrfq"
      },
      "source": [
        "#Q2:  Feature Activation Normalization Methods\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Feature activation normalization methods (Batch Normalization, Layer Normalization, Instance Normalization, Group Normalization, etc.) are introduced to overcome 'Internal Covariate Shift', defined as the change in the distribution of network activations due to the change in the network parameters during training.\n",
        "\n",
        "In this practice, we will implement only forward-pass functions for each normalization method.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1nvuJ2GPsd8Q59XQdcIqopPmSj5XwLZpc)\n",
        "\n",
        "Image Source: Wu, Yuxin, and Kaiming He. \"Group normalization.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVQHh0IqQo65"
      },
      "source": [
        "## Batch Normalization\n",
        "\n",
        "The aim of the batch normalization is to reduce internal covariate shift by normalizing the mini-batch activation by the mean and variance of the activations.\n",
        "\n",
        "You can implement Batch Normalization using numpy library with the formulas below:\n",
        "\n",
        "$\\mu_{c} = \\frac{1}{B H W} \\sum_{b=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{b,c,h,w}$\n",
        "\n",
        "$\\sigma^2_{c} = \\frac{1}{B H W} \\sum_{b=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{b,c,h,w} - \\mu_{c})^2 $\n",
        "\n",
        "apply normalization as:\n",
        "\n",
        "$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mu_{c}}{\\sqrt{\\sigma^2_{c} + \\epsilon}}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kQlkIH5XWcH"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def batch_norm(x, epsilon=1e-05):\n",
        "\n",
        "  # Implement batch normalization\n",
        "\n",
        "  #### Solution ####\n",
        "  \n",
        "  \n",
        "  ##################\n",
        "  \n",
        "  return x_normalized\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvKnTVVjXinh"
      },
      "source": [
        "# TEST Your Solutions\n",
        "import torch\n",
        "\n",
        "# An input feature map with B, C, H, W dimensions\n",
        "batch_size = 20\n",
        "channels = 100\n",
        "height = 35\n",
        "width = 45\n",
        "x = np.random.rand(batch_size, channels, height, width)\n",
        "\n",
        "x_normalized = batch_norm(x)\n",
        "x_normalized_ground_truth = torch.nn.BatchNorm2d(channels, affine=False)(torch.Tensor(x)).numpy()\n",
        "\n",
        "# The results can have small numerical differences between numpy and pytorch implementations\n",
        "print(f\"Are the results close enough: {np.allclose(x_normalized, x_normalized_ground_truth, atol=1e-5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m19OLfStd1Wq"
      },
      "source": [
        "## Layer Normalization\n",
        "\n",
        "Instead of normalizing examples across mini-batches, layer normalization normalizes each example in the batch across its dimensions. For input $x$ with shape [B, C, H, W], it computes\n",
        "\n",
        "$\\mu_{b} = \\frac{1}{C H W} \\sum_{c=1}^{C} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{b,c,h,w}$\n",
        "\n",
        "$\\sigma^2_{b} = \\frac{1}{C H W} \\sum_{c=1}^{C} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{b,c,h,w} - \\mu_{b})^2 $\n",
        "\n",
        "apply normalization as:\n",
        "\n",
        "$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mu_{b}}{\\sqrt{\\sigma^2_{b} + \\epsilon}}$\n",
        "\n",
        "Implement Layer Normalization using numpy library with the formulas given above.\n",
        "\n",
        "Source: Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer normalization.\" arXiv preprint arXiv:1607.06450 (2016).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHgKlqZ0a8Z1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def layer_norm(x, epsilon=1e-05):\n",
        "\n",
        "  # Implement layer normalization\n",
        "\n",
        "  #### Solution ####\n",
        "  \n",
        "  ##################\n",
        "  \n",
        "  return x_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0mxC_65ACyo"
      },
      "source": [
        "# TEST Your Solutions\n",
        "import torch\n",
        "\n",
        "# An input feature map with B, C, H, W dimensions\n",
        "batch_size = 20\n",
        "channels = 100\n",
        "height = 35\n",
        "width = 45\n",
        "x = np.random.rand(batch_size, channels, height, width)\n",
        "\n",
        "x_normalized = layer_norm(x)\n",
        "x_normalized_ground_truth = torch.nn.LayerNorm(x.shape[1:], elementwise_affine=False)(torch.Tensor(x)).numpy()\n",
        "\n",
        "# The results can have small numerical differences between numpy and pytorch implementations\n",
        "print(f\"Are the results close enough: {np.allclose(x_normalized, x_normalized_ground_truth, atol=1e-5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s6JWU_tBae7"
      },
      "source": [
        "## Instance Normalization\n",
        "\n",
        "Instead of normalizing examples across mini-batches or channels, instance normalization normalizes each features within each channel of each batch instance. For input ùë• with shape [B, C, H, W], it computes\n",
        "\n",
        "$\\mu_{b,c} = \\frac{1}{H W} \\sum_{w=1}^{W} \\sum_{h=1}^{H} x_{b,c,h,w}$\n",
        "\n",
        "$\\sigma^2_{b,c} = \\frac{1}{H W} \\sum_{w=1}^{W} \\sum_{h=1}^{H} (x_{b,c,h,w} - \\mu_{b,c})^2$\n",
        "\n",
        "apply normalization as:\n",
        "\n",
        "$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mu_{b,c}}{\\sqrt{\\sigma^2_{b,c} + \\epsilon}}$\n",
        "\n",
        "Implement Layer Normalization using numpy library with the formulas given above.\n",
        "\n",
        "Source: Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Instance normalization: The missing ingredient for fast stylization.\" arXiv preprint arXiv:1607.08022 (2016).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OySow-jMBZoA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def instance_norm(x, epsilon=1e-05):\n",
        "\n",
        "  # Implement instance normalization\n",
        "\n",
        "  #### Solution ####\n",
        "\n",
        "  ##################\n",
        "  \n",
        "  return x_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAXXsPY4Ar6k"
      },
      "source": [
        "# TEST Your Solutions\n",
        "import torch\n",
        "\n",
        "# An input feature map with B, C, H, W dimensions\n",
        "batch_size = 20\n",
        "channels = 100\n",
        "height = 35\n",
        "width = 45\n",
        "x = np.random.rand(batch_size, channels, height, width)\n",
        "\n",
        "x_normalized = instance_norm(x)\n",
        "x_normalized_ground_truth = torch.nn.InstanceNorm2d(batch_size, affine=False)(torch.Tensor(x)).numpy()\n",
        "\n",
        "# The results can have small numerical differences between numpy and pytorch implementations\n",
        "print(f\"Are the results close enough: {np.allclose(x_normalized, x_normalized_ground_truth, atol=1e-5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88w5CVe0euGm"
      },
      "source": [
        "## Group Normalization\n",
        "\n",
        "Instead of normalizing examples across mini-batches or channels, group normalization split channel dimension into groups and normalizes inputs across these groups. For input ùë• with shape [B, C, H, W] and number of groups $G$, we split $x$ to the shape [B, G, C<sub>G</sub>, H, W] (C<sub>G</sub> is $C / G$) and compute normalization parameters as:\n",
        "\n",
        "$\\mu_{b,g} = \\frac{1}{C_G H W} \\sum_{c_g=1}^{C_G}\\sum_{w=1}^{W} \\sum_{h=1}^{H} x_{b,g,c_g,h,w}$\n",
        "\n",
        "$\\sigma^2_{b,g} = \\frac{1}{C_G H W} \\sum_{c_g=1}^{C_G}\\sum_{w=1}^{W} \\sum_{h=1}^{H} (x_{b,g,c_g,h,w} - \\mu_{b,g})^2$\n",
        "\n",
        "apply normalization as:\n",
        "\n",
        "$\\hat{x}_{b,g,c_g,h,w} = \\frac{x_{b,g,c_g,h,w} - \\mu_{b,g}}{\\sqrt{\\sigma^2_{b,g} + \\epsilon}}$\n",
        "\n",
        "Implement Layer Normalization using numpy library with the formulas given above.\n",
        "\n",
        "Source: Wu, Yuxin, and Kaiming He. \"Group normalization.\" Proceedings of the European conference on computer vision (ECCV). 2018."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXVR1mCEap1W"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def group_norm(x, num_groups, epsilon=1e-05):\n",
        "\n",
        "  # Implement group normalization\n",
        "\n",
        "  #### Solution ####\n",
        "  \n",
        "  ##################\n",
        "  \n",
        "  return x_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxqAoEW1n_-Q"
      },
      "source": [
        "# TEST Your Solutions\n",
        "import torch\n",
        "\n",
        "# An input feature map with B, C, H, W dimensions\n",
        "batch_size = 20\n",
        "channels = 100\n",
        "height = 35\n",
        "width = 45\n",
        "x = np.random.rand(batch_size, channels, height, width)\n",
        "\n",
        "num_groups = 5\n",
        "\n",
        "x_normalized = group_norm(x, num_groups)\n",
        "x_normalized_ground_truth = torch.nn.GroupNorm(num_groups, batch_size, affine=False)(torch.Tensor(x)).numpy()\n",
        "\n",
        "# The results can have small numerical differences between numpy and pytorch implementations\n",
        "print(f\"Are the results close enough: {np.allclose(x_normalized, x_normalized_ground_truth, atol=1e-5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS28mA2Lzzzv"
      },
      "source": [
        "#Q3:  Layers\n",
        "\n",
        "Layers are main building blocks of neural networks. Basically, a layer is a tensor-in tensor-out operation that can also contain state informations such as weights and bias variables. Dense layers, locally connected layers, pooling layers and convolutional layers are among the most used. In this practise, we will implement fully connected and convolutional layers. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT_F1uX0JrJW"
      },
      "source": [
        "## Building A Fully Connected Network\n",
        "\n",
        "Using high-level frameworks like Tensorflow or Pytorch, It is possible create complex deep learning models quickly. However, it is worth taking the time to look inside and understand underlying concepts. Therefore, we will try to utilize our knowledge and build a fully operational neural network using only NumPy library. Finally, we will also test our model and solve a simple classification problem. \n",
        "\n",
        "Our goal is to create a program capable of creating a densely connected neural network with the specified architecture (number and size of layers and appropriate activation function)\n",
        "\n",
        "![](https://drive.google.com/uc?id=1f-0py75nR_ZTgNkOjuj9aIeSze1Kk9nU)\n",
        "\n",
        "The most challenging part of our task is to build right data structures and and manage the initial states as we train our network. We will need all the initial states and intermediate values to calculate gradients as you can see in the image below.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1cHXTSG1C6xjXsY3Yqfk6XRwCXJ7Q-Vhk)\n",
        "\n",
        "\n",
        "We will implement a class that will forward propagate an input, save initial states and use these initial states as we backpropagate and update the weight and bias terms.\n",
        "\n",
        "The network architecture is defined as a list of dictionaries. Each item in the list describes the necessary information about the layer:\n",
        "\n",
        "1. input_dim - the size of the signal vector supplied as an input for the layer, \n",
        "2. output_dim - the size of the activation vector obtained at the output of the layer\n",
        "3. activation - the activation function to be used inside the layer.\n",
        "\n",
        "An example network architecture:\n",
        "\n",
        "```\n",
        "architecture = [\n",
        "    {\"input_dim\": 2, \"output_dim\": 32, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 32, \"output_dim\": 64, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 16, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
        "]\n",
        "```\n",
        "\n",
        "We will use stochastic gradient descent optimization, which means we will update weights as:\n",
        "\n",
        "$dW_{l} := W_{l} - \\eta\\, \\nabla{W_{l}}$\n",
        "\n",
        "where $\\eta$ is our learning rate.\n",
        "\n",
        "To calculate gradients for each layer, You can use formulas below:\n",
        "\n",
        "$dW_{l} = \\frac{\\nabla L}{\\nabla W_{l}} = \\frac{1}{N} \\, dZ_{l} \\, A_{l-1}^T$\n",
        "\n",
        "$db_{l} = \\frac{\\nabla L}{\\nabla b_{l}} = \\frac{1}{N} \\sum_{i=1}^{N}dZ_{l}^{(i)}$\n",
        "\n",
        "$dA_{l-1} = \\frac{\\nabla L}{\\nabla A_{l-1}} = W_{l}^{T} \\, dZ_{l}$\n",
        "\n",
        "$dZ_{l} = dA_{l} \\, g'(Z_{l})$\n",
        "\n",
        "But we will start backward propagation by calculating the derivative of the loss with respect to the prediction vector. \n",
        "\n",
        "We will use binary crossentropy as out cost function:\n",
        "\n",
        "$L(\\hat{y}, y) = y \\, \\log(\\hat{y}) + (1-y) \\, \\log(1 - \\hat{y})$\n",
        "\n",
        "where $y$ is the ground truth labels and $\\hat{y}$ is the predictions made by model.\n",
        "\n",
        "The calculation of the derivative of the loss with respect to the prediction vector is left as an exersice. \n",
        "\n",
        "From now on, You can follow the instructions inside each function and complete necessary parts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmuFuvaAO0V7"
      },
      "source": [
        "# Implement a fully connected network with given architecture as a dictionary.\n",
        "# Hidden layers will have ReLU activation functions and the output will have sigmoid activations.\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "architecture = [\n",
        "    {\"input_dim\": 2, \"output_dim\": 32, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 32, \"output_dim\": 64, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 16, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
        "]\n",
        "\n",
        "\n",
        "class FullyConnectedNetwork:\n",
        "    def __init__(self, architecture):\n",
        "        # These are the parameters we will need throughtout the network training process\n",
        "        self.architecture = architecture\n",
        "        self.num_layers = len(architecture)\n",
        "        self.params = {}\n",
        "\n",
        "        # We will use these dictionaries to save initial states\n",
        "        self.output_memory = {}\n",
        "        self.grad_memory = {}\n",
        "\n",
        "\n",
        "        for layer_idx, layer in enumerate(self.architecture):\n",
        "            layer_input_size = layer[\"input_dim\"]\n",
        "            layer_output_size = layer[\"output_dim\"]\n",
        "            layer_activation = layer[\"activation\"]\n",
        "\n",
        "            \n",
        "            self.params[f\"input_dim_{layer_idx}\"] = layer_input_size\n",
        "            self.params[f\"output_dim_{layer_idx}\"] = layer_output_size\n",
        "            self.params[f\"activation_{layer_idx}\"] = layer_activation\n",
        "            # Initialize weight and bias terms\n",
        "\n",
        "            ### Solution ###\n",
        "            # Weigth terms can be initialized with random values\n",
        "            self.params[f\"w_{layer_idx}\"] = ###\n",
        "            # Bias can be initialized with zeroes\n",
        "            self.params[f\"b_{layer_idx}\"] = ###\n",
        "            ################\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Implement sigmoid function\n",
        "        # Hint: sigmoid(x) = 1 / (1 + e^(-x))\n",
        "        ### Solution ###\n",
        "        return \n",
        "        ################\n",
        "\n",
        "    def relu(self, x):\n",
        "        # Implement relu function\n",
        "        # Hint relu function returns x if x > 0, 0 otherwise\n",
        "        ### Solution ###\n",
        "\n",
        "        return ###\n",
        "        ################\n",
        "\n",
        "    def binary_cross_entropy_loss(self, y_pred, y_true):\n",
        "        # We will use binary crossentropy as our cost function\n",
        "        # loss(y_pred, y_true) = mean(-(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))))\n",
        "\n",
        "        ### Solution ###\n",
        "\n",
        "        return ###\n",
        "        ################\n",
        "\n",
        "    def forward_propagation_single_layer(self, x, layer_idx):\n",
        "        # Implement forward propagation function for a single layer\n",
        "        # Output should be a tuple containing (W*x + b) and activation_function(W*x + b)\n",
        "        # This tuple will be useful because we will be needing output without activation function in the backward propagation step\n",
        "        # Since we have already initialized weight and bias variables, you can use self.params dictionary with the given layer_idx\n",
        "        # You will need w_{layer_idx}, b_{layer_idx} and activation_{layer_idx} terms\n",
        "\n",
        "        ### Solution ###\n",
        "        \n",
        "        ################\n",
        "\n",
        "    def forward_propagation_network(self, x):\n",
        "        # Impelent forward propagation function for whole network\n",
        "        # Output of each layer should be stored in memory so that we can use them in back propagation\n",
        "        # Return network output\n",
        "\n",
        "        current_x = x\n",
        "\n",
        "        # Save model input values as the layer -1 outputs\n",
        "        self.output_memory[f\"z_{-1}\" ] = current_x\n",
        "        self.output_memory[f\"z_activation_{-1}\"] = current_x\n",
        "\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            # Implement forward propagation for the whole network using the function you implemented above \"forward_propagation_single_layer\"\n",
        "            # And save the intermediate outputs to self.output_memory dictionary\n",
        "            ### Solution ###\n",
        "            z, z_activation = ###\n",
        "\n",
        "            self.output_memory[f\"z_{layer_idx}\"] = z\n",
        "            self.output_memory[f\"z_activation_{layer_idx}\"] = z_activation\n",
        "\n",
        "            current_x = z_activation\n",
        "            #################\n",
        "\n",
        "        return z_activation\n",
        "\n",
        "    def backward_relu(self, d, z):\n",
        "        # Implement backward function ReLU activation function\n",
        "        # dz = d * (0 if z <=0, 1 elsewhere)\n",
        "        # Hint: You cane use np.where function\n",
        "\n",
        "        ### Solution ###\n",
        "        \n",
        "        ################\n",
        "\n",
        "    def backward_sigmoid(self, d, z):\n",
        "        # Implement backward function sigmoid activation function\n",
        "        # dz = d * sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "        ### Solution ###\n",
        "        \n",
        "        ################\n",
        "\n",
        "    def backward_propagation_single_layer(self, layer_idx, diff):\n",
        "        # Implement gradients for a single layer\n",
        "        # Formulas are given in the description of the problem\n",
        "        # You will need self.output_memory dictionary, \n",
        "        # specifically activation_{layer_idx}, z_{layer_idx}, z_activation_{layer_idx-1} items\n",
        "        # self.params will be useful to get input dimension of the layer: input_dim_{layer_idx}\n",
        "        \n",
        "        # You will calculate d, dw and db\n",
        "\n",
        "        ### Solution ###\n",
        "        if self.params[f\"activation_{layer_idx}\"] is \"relu\":\n",
        "            dz = ###\n",
        "        elif self.params[f\"activation_{layer_idx}\"] is \"sigmoid\":\n",
        "            dz = ###\n",
        "        else:\n",
        "            raise Exception(\"Non-supported activation function\")\n",
        "\n",
        "        dw = ###\n",
        "        db = ###\n",
        "        d = ###\n",
        "        #################\n",
        "\n",
        "        return d, dw, db\n",
        "        \n",
        "    \n",
        "    def backward_propagation_network(self, y_true):\n",
        "        #calculate gradient for loss \n",
        "        # dloss(y_true) = - ((y_true / y_pred) - ((1-y_true)/ 1 - y_pred))\n",
        "\n",
        "        y_pred = self.output_memory[f\"z_activation_{self.num_layers -1}\"]\n",
        "        # Implement derivative of the loss function with respect to prediction vector y_pred\n",
        "        ### Solution ###\n",
        "        d = ###\n",
        "        ################\n",
        "        for layer_idx in reversed(range(self.num_layers)):\n",
        "            \n",
        "            d, dw, db = self.backward_propagation_single_layer(layer_idx, d)\n",
        "\n",
        "            self.grad_memory[f\"dw_{layer_idx}\" ] = dw\n",
        "            self.grad_memory[f\"db_{layer_idx}\"] = db\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            # Update layer parameters with the gradients you calculated.\n",
        "            # You will need self.grad_memory: dw_{layer_idx}, db_{layer_idx}\n",
        "            # You will need self.params: w_{layer_idx}, b_{layer_idx}\n",
        "\n",
        "            ### Solution ###\n",
        "            self.params[f\"w_{layer_idx}\"] = ###   \n",
        "            self.params[f\"b_{layer_idx}\"] = ###\n",
        "            ################\n",
        "\n",
        "    def train(self, x, y_true, epoch, learning_rate):\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        for i in range(epoch):\n",
        "            y_pred = self.forward_propagation_network(x)\n",
        "            loss = self.binary_cross_entropy_loss(y_pred, y_true)\n",
        "            losses.append(loss)\n",
        "            accuracies.append(np.mean(np.round(y_pred) == y_true))\n",
        "            self.backward_propagation_network(y_true)\n",
        "            self.update(learning_rate)\n",
        "\n",
        "        return losses, accuracies\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo0MN05F6j8a"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nn = FullyConnectedNetwork(architecture)\n",
        "\n",
        "n_samples = 100\n",
        "x,y = make_moons(n_samples=n_samples)\n",
        "y = np.expand_dims(y, axis=-1)\n",
        "\n",
        "losses, accuracies = nn.train(x, y_true=y, epoch=1000, learning_rate=0.1)\n",
        "\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.plot(accuracies)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2U5Jm6lJUdu"
      },
      "source": [
        "## Convolutional Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol9uflTGPDvu"
      },
      "source": [
        "# Implement a function that applies a filter to input image\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "\n",
        "\n",
        "def convolve_filter(image, filter):\n",
        "  # Apply filter to image\n",
        "  # Image will be a grayscale image and filter will be of shape (m x n)\n",
        "\n",
        "  ### Solution ###\n",
        "\n",
        "  ################\n",
        "  return output_image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhhOu-YSGoxc"
      },
      "source": [
        "# We will apply sobel filter to a grayscale image using the convolve_filter function you implemented above\n",
        "# Sobel filter, is used in image processing, particularly within edge detection algorithms where it creates an image emphasising edges\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sobel_filter_x = np.array([[-1,0,1],\n",
        "                         [-2,0,2],\n",
        "                         [-1,0,1]])\n",
        "\n",
        "sobel_filter_y = np.array([[-1,-2,-1],\n",
        "                         [0,0,0],\n",
        "                         [1,2,1]])\n",
        "\n",
        "lena_grayscale = imread(\"https://www.cosy.sbg.ac.at/~pmeerw/Watermarking/lena_gray.gif\")\n",
        "plt.imshow(lena_grayscale, cmap='gray')\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()\n",
        "output_x = convolve_filter(lena_grayscale, sobel_filter_x)\n",
        "output_y = convolve_filter(lena_grayscale, sobel_filter_y)\n",
        "plt.title(\"Sobel X Output\")\n",
        "plt.imshow(output_x, cmap='gray')\n",
        "plt.show()\n",
        "plt.title(\"Sobel Y Output\")\n",
        "plt.imshow(output_y, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPuJZTuMJduQ"
      },
      "source": [
        "#Q4: Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3UHhqTHTIN"
      },
      "source": [
        "Naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features. We will use a Gaussian Naive Bayes classifier because when dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution.\n",
        "\n",
        "We will basically frame the classification problem with Bayes theorem as follows:\n",
        "\n",
        "$P(y_i \\vert x_1, x_2, \\dots , x_n) = \\frac{P(x_1, x_2, \\dots, x_n \\vert y_i) * P(y_i)}{P(x1, x2, ‚Ä¶, xn)}$\n",
        "\n",
        "(To make calculations a little straightforward, we will ignore the denominator part since it will be the same for all of the inputs)\n",
        "\n",
        "We will follow these steps:\n",
        "1. Separate Instances By Class\n",
        "2. Summarize Data By Class\n",
        "3. Gaussian Probability Density Function\n",
        "4. Make Predictions\n",
        "5. Create New Instances For Each Class \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD50uUdID4uk"
      },
      "source": [
        "### Separate Instances By Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1VhvSxDx81"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def separate_data_by_class(x, y):\n",
        "    # Get Unique class labels\n",
        "    classes = np.unique(y)\n",
        "    # Populate class instances dictionary with (class_label : class_instance_list)\n",
        "    class_instances = {}\n",
        "    for c in classes:\n",
        "        class_instances[c] = []\n",
        "    for x_i, y_i in zip(x,y):\n",
        "        class_instances[y_i].append(x_i)\n",
        "    return class_instances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyb1VizJD_Ne"
      },
      "source": [
        "### Summarize Data By Class\n",
        "\n",
        "We need mean and variance for each column of the input instances to calculate Gaussian Probability Density Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJxIespWD3s0"
      },
      "source": [
        "def mean(x):\n",
        "    # Implement a function to calculate mean for each feature\n",
        "    # Hint: you can use numpy mean function with \"axis\" parameter\n",
        "\n",
        "    ### Solution ###\n",
        "    \n",
        "    ################\n",
        "\n",
        "def standart_deviation(x):\n",
        "    # Implement a function to calculate standart deviation for each feature\n",
        "\n",
        "    ### Solution ###\n",
        "    \n",
        "    ################\n",
        "\n",
        "def calculate_class_summary(class_instances):\n",
        "    class_summary = {}\n",
        "    # Calculate mean and variance for each class\n",
        "    ### Solution ###\n",
        "    for c, feature in class_instances.items():\n",
        "        class_summary[c] = {\"mean\": ###,\n",
        "                            \"std_dev\": ###}\n",
        "    ################\n",
        "    return class_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh839PkxHoDW"
      },
      "source": [
        "### Gaussian Probability Density Function\n",
        "\n",
        "We will implement function to calculate a Gaussian Probability Density function\n",
        "\n",
        "$g(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtNvCrLTHlIk"
      },
      "source": [
        "def gaussian_probability_density(x, mean, std_dev):\n",
        "  ### Solution ###\n",
        "  \n",
        "  return ###\n",
        "  ################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Epdo3PMLb8"
      },
      "source": [
        "def predict(x, summaries):\n",
        "  # Calculate probabilities for each class and make predictions using the max probability value for each instance\n",
        "\n",
        "  ### Solution ###\n",
        "\n",
        "  return ###\n",
        "  ################\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "  # Calculate classification accuracy \n",
        "  \n",
        "  ### Solution ###\n",
        "\n",
        "  return ###\n",
        "  ################\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaEHSS8wSrNc"
      },
      "source": [
        "Test the implementations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1LMXJNiCTBY"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "x, y = make_blobs(n_samples=500, centers=2, n_features=2)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "class_instances = separate_data_by_class(x_train, y_train)\n",
        "class_summary = calculate_class_summary(class_instances)\n",
        "predictions = predict(x_test, class_summary)\n",
        "print(accuracy(predictions, y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA0xdn2HTw5I"
      },
      "source": [
        "## Since we know (or we assume that we know) class distributions we can sample new instances for each class\n",
        "## A nice example for a generative model\n",
        "\n",
        "#Create 10 new instance from the summaries calculated above\n",
        "new_class_instances = {}\n",
        "for c, summary in class_summary.items():\n",
        "  #Hint: Use np.random.normal\n",
        "  ### Solution ###\n",
        "  new_class_instances[c] = ###\n",
        "  ################\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for c, instances in class_instances.items():\n",
        "  instances = np.array(instances)\n",
        "  plt.scatter(instances[:,0], instances[:,1], label=f\"Label {c}\")\n",
        "for c, instances in new_class_instances.items():\n",
        "  instances = np.array(instances)\n",
        "  plt.scatter(instances[:,0], instances[:,1], label=f\"New Instance for label{c}\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4LFeSU9tYtO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}