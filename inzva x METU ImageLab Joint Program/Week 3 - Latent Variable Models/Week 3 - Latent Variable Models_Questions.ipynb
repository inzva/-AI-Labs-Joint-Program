{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 3 - Latent Variable Models.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1jZaoMjcXFS3pSU3o6JW6cd0EsuPyxOf5","authorship_tag":"ABX9TyNZOV2iwmZ0y2g7W2WQg0/l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2b8c9e88499c4fa2b027721a7539353b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5730c8b701084be79b5d61c5ebb3508c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0d62b31e3b74a67a93bebec76192fca","IPY_MODEL_a6e0dc840a0e453e8bdaac3086bb1cbe"]}},"5730c8b701084be79b5d61c5ebb3508c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0d62b31e3b74a67a93bebec76192fca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ac2c68504acb4245a08a421e11e0bc52","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15ef1c814f9042d6831137dc2c4cf5e2"}},"a6e0dc840a0e453e8bdaac3086bb1cbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e4ccacf86119416b9229bea3137ad9f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [03:00&lt;00:00, 54962.42it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8be415e9743e471ba29456aa7162ed5e"}},"ac2c68504acb4245a08a421e11e0bc52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"15ef1c814f9042d6831137dc2c4cf5e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4ccacf86119416b9229bea3137ad9f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8be415e9743e471ba29456aa7162ed5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2c6fe190d6648499f6c96a4d1a51f02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1d734f8698754338adf1dd961e97ad77","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f3319b756455403395a9f0e2fad3d102","IPY_MODEL_6f06c36241d742389f9a8ba1285cd2e6"]}},"1d734f8698754338adf1dd961e97ad77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f3319b756455403395a9f0e2fad3d102":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_89875c6d812c4aa897a6ff485c80e2d2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc797ff04680461fb229fbaa9e764812"}},"6f06c36241d742389f9a8ba1285cd2e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_55eaf391fc6e4aadabce3d0465e3a4a8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:00&lt;00:00, 139803.07it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e3aa24e8faf644f4aca596ec5cbca24d"}},"89875c6d812c4aa897a6ff485c80e2d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cc797ff04680461fb229fbaa9e764812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"55eaf391fc6e4aadabce3d0465e3a4a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e3aa24e8faf644f4aca596ec5cbca24d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05961366639448068e6098957ef18432":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a0d28060b7c847c894abf2277928d653","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dc2c288b0a5a421b945aa06e97bb525c","IPY_MODEL_3fcadee46d86474ea6c48e5711b64ea8"]}},"a0d28060b7c847c894abf2277928d653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dc2c288b0a5a421b945aa06e97bb525c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4fb111ef403d442086cbfe84f642826b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be8ed9a6be7f4d649d9229e03d5ee3f1"}},"3fcadee46d86474ea6c48e5711b64ea8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a304934a37545d5b5c135d9f2778b14","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:24&lt;00:00, 67282.55it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b442fdc235184cf0b45803fac2303e17"}},"4fb111ef403d442086cbfe84f642826b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"be8ed9a6be7f4d649d9229e03d5ee3f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a304934a37545d5b5c135d9f2778b14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b442fdc235184cf0b45803fac2303e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"618c1d51f2ab4e11a814ea35ea064081":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1cf1a20be9cf4475b5654b11980c24c9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_59df369af7c54d958c18dcaaefbaaa81","IPY_MODEL_215535f8b855428eaae93f2a63da15ae"]}},"1cf1a20be9cf4475b5654b11980c24c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59df369af7c54d958c18dcaaefbaaa81":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1bc7b456bfc1444bb334d37faf41d371","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4831b6f7db6d4d83b2e263734d07cfe5"}},"215535f8b855428eaae93f2a63da15ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2129b17be0de4e4db540e24a39c001a1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:11&lt;00:00, 452.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f329a5f208094637bd7405d73610264a"}},"1bc7b456bfc1444bb334d37faf41d371":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4831b6f7db6d4d83b2e263734d07cfe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2129b17be0de4e4db540e24a39c001a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f329a5f208094637bd7405d73610264a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"ILnbS8-tkwl1"},"source":["Answer the following questions:\n","\n","\n","\n","1. What are the advantages and disadvantages of Latent Variable Models? Explain.\n","2. Explain the differences between Forward KL and Backward KL divergences? Which is used in ELBO maximization and why? \n","3. Why do we use variational inference instead of Maximum likelihood estimation or expectation maximization techniques to maximize the likelihood, $p_{\\boldsymbol{\\theta}}(\\mathbf{X})$? \n","4. Why do we maximize the Evidence Lower Bound (ELBO) to maximize the true marginal likelihood? Explain the two ways of derivation of the ELBO and state the key mathematical concepts in these derivations.\n","5. What is the reason for the need of the reparameterization trick?\n","6. What is amortized inference and what is its advantage? \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PIFQrxioo5JL"},"source":["#VAE Implementation\n","\n","Complete the code where # YOUR CODE HERE has been stated. \n"]},{"cell_type":"markdown","metadata":{"id":"LXtbY-8jgHrO"},"source":["* VAE implementation task is modified from Deep Learning Course exercises at Aalto University."]},{"cell_type":"code","metadata":{"id":"QtGwODxfUYR-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628442817923,"user_tz":-180,"elapsed":26220,"user":{"displayName":"Mine Öğretir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUw_J84inQWrMSAiy-BcWpjAKjNykcApwB_0VxrQ=s64","userId":"08717881441939214501"}},"outputId":"05459386-4285-4c68-a24f-fe9c427dcb0f"},"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","## Update directory \n","base_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Latent Variable Models'\n","sys.path.insert(0, base_dir)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3KcWENukm4wj","executionInfo":{"status":"ok","timestamp":1628442823690,"user_tz":-180,"elapsed":5774,"user":{"displayName":"Mine Öğretir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUw_J84inQWrMSAiy-BcWpjAKjNykcApwB_0VxrQ=s64","userId":"08717881441939214501"}}},"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import tools\n","import tests"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmD0TStopIhc","executionInfo":{"status":"ok","timestamp":1628442823691,"user_tz":-180,"elapsed":8,"user":{"displayName":"Mine Öğretir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUw_J84inQWrMSAiy-BcWpjAKjNykcApwB_0VxrQ=s64","userId":"08717881441939214501"}}},"source":["skip_training = False\n","data_dir = './data'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"21AmFVyApb3n","executionInfo":{"status":"ok","timestamp":1628442823692,"user_tz":-180,"elapsed":7,"user":{"displayName":"Mine Öğretir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUw_J84inQWrMSAiy-BcWpjAKjNykcApwB_0VxrQ=s64","userId":"08717881441939214501"}}},"source":["#device = torch.device('cuda:0')\n","device = torch.device('cpu')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzEAgWbJpjIb","colab":{"base_uri":"https://localhost:8080/","height":539,"referenced_widgets":["2b8c9e88499c4fa2b027721a7539353b","5730c8b701084be79b5d61c5ebb3508c","e0d62b31e3b74a67a93bebec76192fca","a6e0dc840a0e453e8bdaac3086bb1cbe","ac2c68504acb4245a08a421e11e0bc52","15ef1c814f9042d6831137dc2c4cf5e2","e4ccacf86119416b9229bea3137ad9f9","8be415e9743e471ba29456aa7162ed5e","d2c6fe190d6648499f6c96a4d1a51f02","1d734f8698754338adf1dd961e97ad77","f3319b756455403395a9f0e2fad3d102","6f06c36241d742389f9a8ba1285cd2e6","89875c6d812c4aa897a6ff485c80e2d2","cc797ff04680461fb229fbaa9e764812","55eaf391fc6e4aadabce3d0465e3a4a8","e3aa24e8faf644f4aca596ec5cbca24d","05961366639448068e6098957ef18432","a0d28060b7c847c894abf2277928d653","dc2c288b0a5a421b945aa06e97bb525c","3fcadee46d86474ea6c48e5711b64ea8","4fb111ef403d442086cbfe84f642826b","be8ed9a6be7f4d649d9229e03d5ee3f1","0a304934a37545d5b5c135d9f2778b14","b442fdc235184cf0b45803fac2303e17","618c1d51f2ab4e11a814ea35ea064081","1cf1a20be9cf4475b5654b11980c24c9","59df369af7c54d958c18dcaaefbaaa81","215535f8b855428eaae93f2a63da15ae","1bc7b456bfc1444bb334d37faf41d371","4831b6f7db6d4d83b2e263734d07cfe5","2129b17be0de4e4db540e24a39c001a1","f329a5f208094637bd7405d73610264a"]},"executionInfo":{"status":"ok","timestamp":1628443029088,"user_tz":-180,"elapsed":205402,"user":{"displayName":"Mine Öğretir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUw_J84inQWrMSAiy-BcWpjAKjNykcApwB_0VxrQ=s64","userId":"08717881441939214501"}},"outputId":"c08386fa-444f-4d79-cc9c-d084d7adfc7b"},"source":["# We will use varianceMNIST data in this exercise\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Transform to tensor\n","    transforms.Lambda(lambda x: x * torch.randn_like(x))\n","])\n","\n","trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b8c9e88499c4fa2b027721a7539353b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2c6fe190d6648499f6c96a4d1a51f02","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05961366639448068e6098957ef18432","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"618c1d51f2ab4e11a814ea35ea064081","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"EsybozndpqGt","colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"status":"ok","timestamp":1628443045201,"user_tz":-180,"elapsed":345,"user":{"displayName":"Mine Öğretir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUw_J84inQWrMSAiy-BcWpjAKjNykcApwB_0VxrQ=s64","userId":"08717881441939214501"}},"outputId":"b2670834-c00b-4586-f845-08005615a938"},"source":["images, labels = iter(trainloader).next()\n","tools.plot_images(images[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"4awSqCzvq204"},"source":["# Variational autoencoder (VAE)\n","\n","In this exercise, we will assume the following generative model for the data:\n","* the latent codes are normally distributed:\n","$$\n","  p(z) = \\mathcal{N}(z \\mid 0, I)\n","$$\n","where $I$ is the identity matrix.\n","* the data are produced from the latent codes as follows:\n","$$\n","  p(x \\mid z) =\\mathcal{N}\\left(x \\mid \\mu_x(z), \\:\\text{diag}(\\sigma^2_x(z)) \\right)\n","$$\n","where $\\mu_x(z)$ and $\\sigma^2_x(z)$ are some deterministic functions that we need to learn."]},{"cell_type":"markdown","metadata":{"id":"PMX9L7XCohxZ"},"source":["## Encoder\n","\n","In the cell below, your task is to implement the encoder of a VAE.\n","\n","The two heads are needed to produce two outputs of the encoder:\n","* means $\\mu_z$ of the approximate distribution of the latent code $\\bar z$\n","* log-variance $\\tilde z$ of the approximate distribution of the latent code $z$.\n","To guarantee that the variance is positive, we parameterize it as $\\sigma_z^2 = \\exp(\\tilde z)$.\n"]},{"cell_type":"code","metadata":{"id":"oceWAFC_nQ-p"},"source":["class Encoder(nn.Module):\n","    def __init__(self, n_components):\n","        # YOUR CODE HERE  \n","        \n","    def forward(self, x):\n","        # YOUR CODE HERE\n","\n","    def sample(self, z_mean, z_logvar):\n","        \"\"\"Draw one sample from the posterior of the latent codes described by given parameters.\n","        This is needed for the re-parameterization trick.\n","        \n","        Args:\n","          z_mean of shape (batch_size, n_components): Means of the approximate distributions of the codes.\n","          z_logvar of shape (batch_size, n_components): Log-variance of the approximate distributions of the codes.\n","        \n","        Returns:\n","          z of shape (batch_size, n_components): Drawn samples.\n","        \"\"\"\n","        # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"euVvoAc58PPG"},"source":["def test_Encoder_shapes():\n","    n_components = 10 # dimensional latent space\n","    encoder = Encoder(n_components=n_components)\n","\n","    x = torch.randn(3, 1, 28, 28)\n","    mu, logsigma = encoder(x)\n","    assert mu.shape == torch.Size([3, n_components]), f\"Bad mu.shape: {mu.shape}\"\n","    assert logsigma.shape == torch.Size([3, n_components]), f\"Bad logsigma.shape: {logsigma.shape}\"\n","    print('Success')\n","\n","test_Encoder_shapes()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsgIIsCd8Rgo"},"source":["def test_Encoder_sample():\n","    n_components = 10 # dimensional latent space\n","    encoder = Encoder(n_components=n_components)\n","\n","    z_mean = torch.zeros(3, n_components)\n","    z_logvar = torch.log(2*torch.ones(3, n_components))\n","    z = encoder.sample(z_mean, z_logvar)\n","    assert z.shape == z_mean.shape, f\"Bad z.shape: {z.shape}\"\n","    print('Success')\n","\n","test_Encoder_sample()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m39ezxOxo-WS"},"source":["### Kullback-Leibler divergence loss\n","\n","One term of the loss function minimized during training of a VAE is the Kullback-Leibler divergence between the approximate distribution of the latent codes $q(z) = \\mathcal{N}(z \\mid \\mu_z, \\sigma^2_z)$ and the prior distribution $p(z) = \\mathcal{N}(z \\mid 0, I)$:\n","$$\n","\\frac{1}{N} \\sum_{i=1}^N \\int q(z_i) \\log \\frac{q(z_i)}{p(z_i)} dz_i\n","$$\n","where $N$ is the number of samples (batch size in our implementation).\n","\n","We will implement this loss function in the cell below.\n","\n","Note: Please do **not** use functions from `torch.distributions` module. \n"]},{"cell_type":"code","metadata":{"id":"ttJYGjm6ncW-"},"source":["def loss_kl(z_mean, z_logvar):\n","    \"\"\"\n","    Args:\n","      z_mean of shape (batch_size, n_components): Means of the approximate distributions of the codes.\n","      z_logvar of shape (batch_size, n_components): Log-variance of the approximate distributions of the codes.\n","    \n","    Returns:\n","      loss (torch scalar): Kullback-Leibler divergence.\n","    \"\"\"\n","    # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuRUZfOE8mH_"},"source":["def test_loss_kl():\n","    n_components = 2\n","    z_mean = torch.zeros(3, n_components)\n","    z_logvar = torch.log(2*torch.ones(3, n_components))\n","    loss = loss_kl(z_mean, z_logvar)\n","    expected = torch.tensor(0.3068528175354004)\n","    print('loss:', loss.item())\n","    print('expected:', expected.item())\n","    assert torch.allclose(loss, expected, atol=1e-5), \"loss does not match expected value.\"\n","    print('Success')\n","\n","test_loss_kl()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjWT5KsFo_Jv"},"source":["## Decoder\n","\n","The decoder computes the predictive distribution of the data given latent code $z$ according to our\n","assumed generative model:\n","$$\n","  p(x \\mid z) = \\mathcal{N}\\left(x \\mid \\mu_x(z), \\sigma^2_x(z) \\right)\n","$$\n","where $\\mu_x(z)$ and $\\sigma^2_x(z)$ are some deterministic functions that we need to learn.\n","\n","You can use the following architecture for the decoder:\n","* Fully-connected layer with 250 output features, followed by ReLU\n","* Fully-connected layer with 250 input features, followed by ReLU\n","* `ConvTranspose2d` layer with kernel size 5 with 16 input channels, followed by ReLU\n","* Two heads made of `ConvTranspose2d` layer with kernel size 5 with 6 input channels.\n","\n","The two heads are needed to produce two outputs of the decoder:\n","* means $\\mu_x$ of the predictive distribution of the data\n","* log-variance $\\tilde x$ of the predictive distribution of the data.\n","To guarantee that the variance is positive, we parameterize it as $\\sigma_x^2 = \\exp(\\tilde x)$.\n","\n","**Important:**\n","\n","In practice, learning the proposed generative model is difficult for the varianceMNIST dataset. The problem is that the background pixels have zero variances, which corresponds to infinitely low loss values. Thus, training may concentrate entirely on modeling the variance of the background pixels. To prevent this, we define the minimum allowed value of the predictive variance $\\tilde x$ and save it in the model as\n","```\n","    self.register_buffer('min_logvar', -6 * torch.ones(1))\n","```\n","We need to use `register_buffer` to make sure that the variable is on the same device as the trained parameters of the model. We can use this code in the forward function to limit the predicted variance by `self.min_logvar`:\n","```\n","logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n","```"]},{"cell_type":"code","metadata":{"id":"neXTi9YJnl05"},"source":["class Decoder(nn.Module):\n","    def __init__(self, n_components):\n","        # YOUR CODE HERE\n","\n","    def forward(self, x):\n","        # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvCL0Y0_9Uui"},"source":["def test_Decoder_shapes():\n","    n_components = 10 # dimensional latent space\n","    decoder = Decoder(n_components=n_components)\n","\n","    z = torch.randn(3, n_components)\n","    y_mean, y_logvar = decoder(z)\n","    y_shape = torch.Size([3, 1, 28, 28])\n","    assert y_mean.shape == y_shape, \"Bad shape of y_mean: y_mean.shape={}\".format(y_mean.shape)\n","    assert y_logvar.shape == y_shape, \"Bad shape of y_logvar: y_logvar.shape={}\".format(y_logvar.shape)\n","    print('Success')\n","\n","test_Decoder_shapes()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-9wASJvo_vj"},"source":["### Expected log-likelihood term\n","\n","The second term of the VAE loss function is minus log-likelihood estimated using sample $z_i$ from the approximate distribution $q(z_i)$ of the latent code that corresponds to training example $x_i$.\n","\n","$$\n","- \\int q(z_i) \\log \\mathcal{N}\\left(x_i \\mid \\mu_x(z_i), \\:\\text{diag}(\\sigma^2_x(z_i))\\right) dz_i\n","$$\n","where $\\mathcal{N}(x_i)$ is a multivariate normal distribution over all pixel values of image $x_i$.\n","\n","Your task is to implement that function **without** constant terms\n","$$\n","\\frac{28 \\cdot 28}{2} \\log 2 \\pi\n","$$\n","that do not depend on $\\mu_x(z_i)$ or $\\sigma_x(z_i)$."]},{"cell_type":"code","metadata":{"id":"h7tMzTm7nsPJ"},"source":["def loss_loglik(y_mean, y_logvar, x):\n","    \"\"\"\n","    Args:\n","      y_mean of shape : Predictive mean of the VAE reconstruction of x.\n","      y_logvar of shape : Predictive log-variance of the VAE reconstruction of x.\n","      x of shape : Training samples.\n","    \n","    Returns:\n","      loss (torch scalar): Expected log-likelihood loss.\n","    \"\"\"\n","    # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1OTcaVu9gIO"},"source":["def test_loss_loglik():\n","    y_mean = torch.zeros(1, 1, 28, 28)\n","    y_logvar = torch.log(2*torch.ones(1, 1, 28, 28))\n","    y_logvar[:,:,:14,:] = torch.log(torch.ones(1, 1, 14, 28))\n","\n","    x = torch.zeros(1, 1, 28, 28)\n","    x[:,:,:14,:] = torch.zeros(1, 1, 14, 28)\n","\n","    loss = loss_loglik(y_mean, y_logvar, x)\n","    expected = torch.tensor(135.85682678222656)\n","    \n","    print('loss:', loss)\n","    print('expected:', expected)\n","    assert torch.allclose(loss, expected), \"loss does not match expected value.\"\n","    print('Success')\n","\n","test_loss_loglik()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZW8EAuDV9rzS"},"source":["# Train a variational autoencoder"]},{"cell_type":"code","metadata":{"id":"wGmzmeMgn9oZ"},"source":["# Create a variational autoencoder\n","n_components = 10 # dimensional latent space\n","encoder = Encoder(n_components=n_components)\n","decoder = Decoder(n_components=n_components)\n","\n","encoder = encoder.to(device)\n","decoder = decoder.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32p7jzi1oVOl"},"source":["### Training loop\n","\n","Implement the training loop in the cell below. One iteration of the training loop process one mini-batch of data in the following way:\n","* The encoder is used to compute approximate distributions $q(z)$ of the latent codes corresponding to the training samples.\n","* One sample $z_i$ is drawn from each approximate posterior $q(z)$ (use function `Encoder.sample()` for that).\n","* The decoder uses samples $z_i$ to compute the predictive distribution for the training examples.\n","* The minimized loss is the sum of the KL-divergence loss `loss_kl()` and the expected log-likelihood loss `loss_loglik()` defined earlier.\n","\n","Implement the training loop in the cell below. The recommended hyperparameters:\n","* Adam optimizer with learning rate 0.001\n","* Number of epochs: 10 (You can use only 1 epoch while checking your code. It converges nicely even in 1 epoch. After that you can use 10 epochs for model saving.)\n","\n","Hints:\n","- The loss at convergence after 10 epochs should be close to -1760."]},{"cell_type":"code","metadata":{"id":"RNaojvFCn_6r"},"source":["#@title\n","# Training loop\n","if not skip_training:\n","    # YOUR CODE HERE\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ny52Pfhm2Ywq"},"source":["# Save the model \n","if not skip_training:\n","    tools.save_model(encoder, f'{base_dir}/vae_encoder.pth')\n","    tools.save_model(decoder, f'{base_dir}/vae_decoder.pth')\n","else:\n","    encoder = Encoder(n_components=10)\n","    tools.load_model(encoder, f'{base_dir}/vae_encoder.pth', device)\n","\n","    decoder = Decoder(n_components=10)\n","    tools.load_model(decoder, f'{base_dir}/vae_decoder.pth', device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qyaElYeY2gVO"},"source":["tests.visualize_embeddings(lambda x: encoder(x)[0], trainloader, n_samples=1000, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQNsvrWb2iv5"},"source":["# In this cell, we visualize predictive variances of the model for each pixel.\n","# For a well-trained VAE, the variances should capture the shapes of the digits.\n","with torch.no_grad():\n","    dataiter = iter(trainloader)\n","    images, _ = dataiter.next()\n","    z_mean, z_logvar = encoder(images.to(device))\n","    y_mean, y_logvar = decoder(z_mean)\n","\n","    # Visualize some data samples\n","    tools.plot_images(images[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])\n","    # Visualize corresponding predictive variance in the pixel space\n","    tools.plot_images(torch.exp(y_logvar[:8]), ncol=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xfClr5L2sZH"},"source":["# Generate samples from VAE\n","# Note that samples may not be of great quality because we did not optimize the architecture of our VAE.\n","with torch.no_grad():\n","    z = torch.randn((8, 10)).to(device)\n","    x_mean, x_logvar = decoder(z)\n","    x = x_mean + torch.exp(0.5 * x_logvar) * torch.randn_like(x_logvar)\n","    tools.plot_images(x[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])\n","    tools.plot_images(torch.exp(x_logvar[:8]), ncol=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jX90qLJoqAh-"},"source":["Missing: Data, visualizations of the trained model etc."]},{"cell_type":"markdown","metadata":{"id":"zWlnYRy4_ANu"},"source":["# Test the quality of the produced embeddings by classification\n","\n","We will test the quality of the produced encodings by training a classifier using the encoded images."]},{"cell_type":"code","metadata":{"id":"xVQ1XVqS_BH1"},"source":["testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKaqBut1_Dca"},"source":["# Encode data samples using the VAE encoder\n","def encode(dataset, dae):\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False)\n","    with torch.no_grad():\n","        embeddings = []\n","        labels = []\n","        for images, labels_ in dataloader:\n","            mu, logsigma = encoder(images.to(device))\n","            embeddings.append(mu)\n","            labels.append(labels_)\n","\n","        embeddings = torch.cat(embeddings, dim=0)\n","        labels = torch.cat(labels, dim=0)\n","    return embeddings, labels\n","\n","traincodes, trainlabels = encode(trainset, encoder)  # traincodes is (60000, 10)\n","testcodes, testlabels = encode(testset, encoder)  # testcodes is (10000, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jj1cR8o_Goh"},"source":["# Train a simple linear classifier\n","from sklearn.linear_model import LogisticRegression\n","\n","logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', max_iter=400)\n","logreg.fit(traincodes.cpu(), trainlabels.cpu())\n","\n","predicted_labels = logreg.predict(testcodes.cpu())  # (10000,)\n","\n","# Compute accuracy of the linear classifier\n","accuracy = np.sum(testlabels.cpu().numpy() == predicted_labels) / predicted_labels.size\n","print('Accuracy with a linear classifier: %.2f%%' % (accuracy*100))\n","assert accuracy > .8, \"Poor accuracy of the embeddings: classification accuracy is %.2f%%\" % (accuracy*100)\n","print('Success')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbStgR6b_jZf"},"source":[""],"execution_count":null,"outputs":[]}]}